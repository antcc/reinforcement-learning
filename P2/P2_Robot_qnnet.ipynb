{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Luis Antonio Ortega Andrés    \n",
    "Antonio Coín Castro*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3B9yjlUxwxi"
   },
   "source": [
    "# Aprendizaje por Refuerzo\n",
    "\n",
    "Segunda parte de la práctica     \n",
    "16 Mayo 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4JNwh3oxwxo"
   },
   "source": [
    "## Enunciado\n",
    "El siguiente notebook implementa un entorno con un brazo robótico de 2 motores. Las acciones posibles son 3*3 (número de acciones en motor1 * número de acciones en motor 2).\n",
    "\n",
    "Se deberá cambiar y extender el código para implementar un sistema de control óptimo donde el extremo del brazo llegue lo más rápidamente al target (cuadrado azul). El cuadrado rojo puede estar en cualquier posición de la rejilla.\n",
    "\n",
    "Se implementará un sistema basado en Q-learning o SARSA donde **la tabla es sustituida por una red neuronal**.\n",
    "\n",
    "## Cuestiones\n",
    "\n",
    "### Pregunta 1.\n",
    "*¿Qué has necesitado cambiar en la definición del entorno?*\n",
    "\n",
    "Se han hecho una serie de modificaciones sobre el código inicial, que comentamos a continuación.\n",
    "\n",
    "**Sobre el objetivo del juego**\n",
    "\n",
    "Se ha modificado la meta: para considerar que hemos ganado en un episodio, el brazo debe permanecer un número de pasos seguidos dentro del objetivo, no solo tocarlo.\n",
    "\n",
    "**Sobre las variables de estado**\n",
    "\n",
    "- Hemos fijado la variable `state_dim` a 7, que es el número de variables de estado que tiene nuestro agente (ver pregunta 2).\n",
    "- Hemos añadido una variable `steps_in_target` que lleva la cuenta del número de pasos seguidos que lleva el brazo dentro del objetivo. También hemos añadido una variable `threshold` que controla el número mínimo de pasos que debe permanecer el brazo dentro del objetivo para considerar que se ha cumplido la tarea (ver preguntas 2 y 3).\n",
    "- Hemos añadido una variable `norm_dist` que fijamos a la mitad de la longitud del escenario (es decir, a 200). Esto nos servirá para normalizar las distancias y hacer que estén en la misma escala, cosa que es importante de cara a utilizarlas como entrada en la red neuronal.\n",
    "\n",
    "**Sobre el método `reset`**\n",
    "\n",
    "Hemos implementado distintos modos de juego, dependiendo de cómo se reinicia la posición del brazo y del objetivo.\n",
    "\n",
    "- `supereasy`: no se resetea nada. La idea era que el brazo aprendiera a quedarse dentro del objetivo sin salirse.\n",
    "- `easy`: se resetea la posición del brazo a su posición inicial. Pretendíamos que el brazo aprendiese a ir a un sitio concreto lo más rápido posible.\n",
    "- `medium`: se resetea la posición del brazo a una posición aleatoria. La idea es similar al modo anterior, pero intentando que aprenda a llegar a un objetivo partiendo de cualquier sitio.\n",
    "- `hard`: se resetea la posición del *target* a una posición aleatoria (pero alcanzable por el brazo). Este es el modo por defecto y el que consideramos como objetivo de la práctica.\n",
    "- `follow`: se desplaza ligeramente la posición del *target*. La idea aquí era simular el comportamiento de un humano desplazando el objetivo con el ratón, de forma que el brazo aprendiera a seguirlo suavemente.\n",
    "\n",
    "**Sobre el método `render`**\n",
    "\n",
    "Hemos modificado este método y el correspondiente de la clase `Viewer`, para que este último reciba las coordenadas del objetivo y pueda volver\n",
    "a pintarlo tras cada reinicio aleatorio.\n",
    "\n",
    "### Pregunta 2.\n",
    "*¿Cómo has formalizado el estado S?*\n",
    "\n",
    "Hemos considerado necesarias 7 variables de estado:\n",
    "\n",
    "- El primer grupo de variables comprende **la posición del objetivo**, representada por la distancia vectorial del mismo al centro del tablero.\n",
    "- El segundo grupo de variables hace referencia a **la posición del antebrazo con respecto al objetivo**. Esto lo medimos como la distancia vectorial del extremo del antebrazo al objetivo.\n",
    "- El tercer grupo de variables hace referencia a **la posición del codo con respecto al objetivo**, medida de nuevo con la distancia vectorial. Estas dos variables no son estrictamente necesarias, pero hemos observado que mejora la estabilidad del entrenamiento (ver pregunta 5).\n",
    "- Finalmente, consideramos una variable binaria (0-1) que simbolice **si el brazo está dentro del objetivo o no** en el paso actual.\n",
    "\n",
    "Notamos que, como comentábamos antes, todas las distancias se encuentran normalizadas y en la misma escala.\n",
    "\n",
    "### Pregunta 3.\n",
    "*¿Cómo has diseñado la recompensa R?*\n",
    "\n",
    "La idea principal de la recompensa es considerar la distancia euclídea entre el objetivo y el extremo más lejano del brazo. En primer lugar, la normalizamos con la misma constante que las variables de estado, y después le cambiamos el signo, de forma que a menor distancia, mayor sea la recompensa.\n",
    "\n",
    "Además de usar la distancia como tal, aumentamos significativamente la recompensa cuando el brazo toque el objetivo para incentivar que lo haga.\n",
    "Se puede comprobar que la recompensa hasta ahora se encuentra en $[-\\sqrt{2}, 0]$, por lo que decidimos sumar $\\sqrt{2}$ a la recompensa si se toca el objetivo. De esta forma, una posición perfecta (el brazo en el centro del objetivo) obtendría una recompensa de $\\sqrt{2}$, que tiene la misma magnitud que la peor recompensa posible ($-\\sqrt{2}$). Además, así nos aseguramos que la recompensa es positiva si y solo si se toca el objetivo.\n",
    "\n",
    "Finalmente, consideramos premiar en gran medida si el brazo consigue mantenerse durante un número de pasos seguidos dentro del objetivo. Concretamente, hemos considerado que se acaba el episodio si se mantiene durante 10 pasos seguidos, y en ese caso sumamos 10 a la recompensa.\n",
    "\n",
    "*Nota:* la manera que tenemos realmente de medir si el brazo está dentro del objetivo es comprobar si se encuentra en el círculo inscrito al cuadrado azul, lo que nos parece una aproximación suficientemente buena y que simplifica los cálculos.\n",
    "\n",
    "### Pregunta 4.\n",
    "*¿Qué parámetros te han dado mejores resultados?*\n",
    "\n",
    "La combinación de parámetros que ha proporcionado los mejores resultados es la siguiente:\n",
    "\n",
    "- Entrenar el modelo durante 200 episodios$^{(\\ast)}$, limitando el número de pasos en cada episodio a 400. De esta forma no tarda demasiado en los episodios iniciales.\n",
    "- Utilizar como red neuronal una red con una capa oculta totalmente conectada de 200 neuronas y activación ReLU, seguida de otra capa completamente conectada con 9 neuronas de salida (tantas como acciones distintas) y activación lineal.\n",
    "- El valor de $\\gamma=0.9$ que hemos usado en muchos ejemplos de clase.\n",
    "- El optimizador Adam con *learning rate* de $0.001$ y pérdida *'mse'*. También hemos probado el SGD con un *learning rate* mayor, pero los resultados no eran mejores.\n",
    "- Utilizar un valor inicial de $\\varepsilon=0.7$, e irlo disminuyendo con un *decay* exponencial multiplicando por $0.99$ tras cada episodio, de forma que al final tenía un valor en torno a $0.1$.\n",
    "- En cuanto a la memoria, la hemos construido con tamaño máximo de 5000, entrenando con un tamaño de *batch* de 100 muestras aleatorias cada 200 pasos (es decir, dos veces por episodio).\n",
    "\n",
    "$(\\ast)$ A la hora de ejecutar el código hemos hecho primero 100 episodios y después otros 100, para comprobar cómo iba el entrenamiento. El tiempo total consumido para los 200 episodios ha sido de aproximadamente 1 hora, utilizando la GPU de Google Colab.\n",
    "\n",
    "Durante el entrenamiento no siempre se alcanza el objetivo en cada episodio, ya que la exploración en la elección de acciones está activa (aunque en los últimos episodios ya se observa que la mayoría de veces lo consigue). Tras el entrenamiento, hemos probado el modelo desactivando completamente la exploración, y ha alcanzado el objetivo en 36 pasos, consiguiendo una recompensa total de 6.52. Si lo ejecutamos en local con el entorno de visualización activado, podemos ver que el brazo ha aprendido a seguir al objetivo de manera aceptable.\n",
    "\n",
    "### Pregunta 5.\n",
    "*¿Qué podrías hacer para minimizar las \"vibraciones\" y/o maximizar la \"suavidad\" en el control del brazo robótico?*\n",
    "\n",
    "Por un lado, hemos observado que añadiendo al estado información sobre la articulación del codo se reducen un poco las vibraciones. Como decíamos, esta información no es estrictamente necesaria para que la red aprenda (al principio estábamos probando sin ellas y funcionaba medianamente bien), pero contribuye a suavizar el movimiento. Probablemente podríamos reducir aún más las vibraciones teniendo en cuenta también los ángulos de ambas articulaciones. \n",
    "\n",
    "Por otro lado, en las primeras pruebas vimos que aunque el brazo conseguía tocar el objetivo, no aprendía a quedarse dentro y oscilaba alrededor. Esto lo arreglamos en gran medida modificando la tarea y haciendo que tuviera que estar una serie de pasos seguidos dentro para ganar, adaptando también las variables de estado y la recompensa. Así el movimiento es mucho más suave y se reducen las vibraciones notablemente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtq3QWjeQCwI"
   },
   "source": [
    "## Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T21:22:04.792342Z",
     "start_time": "2021-05-16T21:22:04.734777Z"
    },
    "id": "GR9EHIkXxwxp"
   },
   "outputs": [],
   "source": [
    "import pyglet\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Viewer(pyglet.window.Window):\n",
    "    color = {\"background\": [1] * 3 + [1]}\n",
    "    fps_display = pyglet.clock.Clock()\n",
    "    bar_thc = 5\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        width,\n",
    "        height,\n",
    "        arm1_coords,\n",
    "        arm2_coords,\n",
    "        arm1_ang,\n",
    "        arm2_ang,\n",
    "        target_coords,\n",
    "        target_width,\n",
    "        mouse_in,\n",
    "    ):\n",
    "        super(Viewer, self).__init__(\n",
    "            width, height, resizable=False, caption=\"Arm\", vsync=False\n",
    "        )  # vsync=False to not use the monitor FPS\n",
    "        self.set_location(x=80, y=10)\n",
    "        pyglet.gl.glClearColor(*self.color[\"background\"])\n",
    "\n",
    "        self.arm1_coords = arm1_coords\n",
    "        self.arm2_coords = arm2_coords\n",
    "        self.arm1_ang = arm1_ang\n",
    "        self.arm2_ang = arm2_ang\n",
    "\n",
    "        self.target_coords = target_coords\n",
    "        self.mouse_in = mouse_in\n",
    "        self.target_width = target_width\n",
    "\n",
    "        self.center_coords = np.array((min(width, height) / 2,) * 2)\n",
    "        self.batch = pyglet.graphics.Batch()\n",
    "\n",
    "        arm1_box, arm2_box, target_box = [0] * 8, [0] * 8, [0] * 8\n",
    "        c1, c2, c3 = (249, 86, 86) * 4, (86, 109, 249) * 4, (249, 39, 65) * 4\n",
    "        self.target = self.batch.add(\n",
    "            4, pyglet.gl.GL_QUADS, None, (\"v2f\", target_box), (\"c3B\", c2)\n",
    "        )\n",
    "        self.arm1 = self.batch.add(\n",
    "            4, pyglet.gl.GL_QUADS, None, (\"v2f\", arm1_box), (\"c3B\", c1)\n",
    "        )\n",
    "        self.arm2 = self.batch.add(\n",
    "            4, pyglet.gl.GL_QUADS, None, (\"v2f\", arm2_box), (\"c3B\", c1)\n",
    "        )\n",
    "\n",
    "    def render(self, arm1_coords, arm2_coords, arm1_ang, arm2_ang, target_coords):\n",
    "        self.arm1_coords = arm1_coords\n",
    "        self.arm2_coords = arm2_coords\n",
    "        self.arm1_ang = arm1_ang\n",
    "        self.arm2_ang = arm2_ang\n",
    "        self.target_coords = target_coords\n",
    "\n",
    "        pyglet.clock.tick()\n",
    "        self._update_arm()\n",
    "        self.switch_to()\n",
    "        self.dispatch_events()\n",
    "        self.dispatch_event(\"on_draw\")\n",
    "        self.flip()\n",
    "\n",
    "    def on_draw(self):\n",
    "        self.clear()\n",
    "        self.batch.draw()\n",
    "        # self.fps_display.draw()\n",
    "\n",
    "    def _update_arm(self):\n",
    "        target_width = self.target_width\n",
    "        target_coords = self.target_coords\n",
    "        target_box = (\n",
    "            target_coords[0] - target_width,\n",
    "            target_coords[1] - target_width,\n",
    "            target_coords[0] + target_width,\n",
    "            target_coords[1] - target_width,\n",
    "            target_coords[0] + target_width,\n",
    "            target_coords[1] + target_width,\n",
    "            target_coords[0] - target_width,\n",
    "            target_coords[1] + target_width,\n",
    "        )\n",
    "        self.target.vertices = target_box\n",
    "\n",
    "        # (x0, y0, x1, y1)\n",
    "        arm1_aux = np.hstack((self.center_coords, self.arm1_coords))\n",
    "        # (x1, y1, x2, y2)\n",
    "        arm2_aux = np.hstack((self.arm1_coords, self.arm2_coords))\n",
    "\n",
    "        arm1_thick_rad = np.pi / 2 - self.arm1_ang\n",
    "        x01 = arm1_aux[0] - np.cos(arm1_thick_rad) * self.bar_thc\n",
    "        y01 = arm1_aux[1] + np.sin(arm1_thick_rad) * self.bar_thc\n",
    "\n",
    "        x02 = arm1_aux[0] + np.cos(arm1_thick_rad) * self.bar_thc\n",
    "        y02 = arm1_aux[1] - np.sin(arm1_thick_rad) * self.bar_thc\n",
    "\n",
    "        x11 = arm1_aux[2] + np.cos(arm1_thick_rad) * self.bar_thc\n",
    "        y11 = arm1_aux[3] - np.sin(arm1_thick_rad) * self.bar_thc\n",
    "\n",
    "        x12 = arm1_aux[2] - np.cos(arm1_thick_rad) * self.bar_thc\n",
    "        y12 = arm1_aux[3] + np.sin(arm1_thick_rad) * self.bar_thc\n",
    "        arm1_box = (x01, y01, x02, y02, x11, y11, x12, y12)\n",
    "\n",
    "        arm2_thick_rad = np.pi / 2 - (self.arm1_ang + self.arm2_ang)\n",
    "        x11_ = arm2_aux[0] + np.cos(arm2_thick_rad) * self.bar_thc\n",
    "        y11_ = arm2_aux[1] - np.sin(arm2_thick_rad) * self.bar_thc\n",
    "\n",
    "        x12_ = arm2_aux[0] - np.cos(arm2_thick_rad) * self.bar_thc\n",
    "        y12_ = arm2_aux[1] + np.sin(arm2_thick_rad) * self.bar_thc\n",
    "\n",
    "        x21 = arm2_aux[2] - np.cos(arm2_thick_rad) * self.bar_thc\n",
    "        y21 = arm2_aux[3] + np.sin(arm2_thick_rad) * self.bar_thc\n",
    "\n",
    "        x22 = arm2_aux[2] + np.cos(arm2_thick_rad) * self.bar_thc\n",
    "        y22 = arm2_aux[3] - np.sin(arm2_thick_rad) * self.bar_thc\n",
    "        arm2_box = (x11_, y11_, x12_, y12_, x21, y21, x22, y22)\n",
    "\n",
    "        self.arm1.vertices = arm1_box\n",
    "        self.arm2.vertices = arm2_box\n",
    "\n",
    "    def on_key_press(self, symbol, modifiers):\n",
    "        if symbol == pyglet.window.key.UP:\n",
    "            self.arm1_ang += 0.1 * 10\n",
    "            print(\n",
    "                self.arm1_coords - self.target_coords,\n",
    "                self.arm1_ang,\n",
    "                self.arm2_coords - self.target_coords,\n",
    "                self.arm2_ang,\n",
    "            )\n",
    "        elif symbol == pyglet.window.key.DOWN:\n",
    "            self.arm1_ang -= 0.1 * 10\n",
    "            print(\n",
    "                self.arm1_coords - self.target_coords,\n",
    "                self.arm1_ang,\n",
    "                self.arm2_coords - self.target_coords,\n",
    "                self.arm2_ang,\n",
    "            )\n",
    "        elif symbol == pyglet.window.key.LEFT:\n",
    "            self.arm2_ang += 0.1\n",
    "            print(\n",
    "                self.arm1_coords - self.target_coords,\n",
    "                self.arm1_ang,\n",
    "                self.arm2_coords - self.target_coords,\n",
    "                self.arm2_ang,\n",
    "            )\n",
    "        elif symbol == pyglet.window.key.RIGHT:\n",
    "            self.arm2_ang -= 0.1\n",
    "            print(\n",
    "                self.arm1_coords - self.target_coords,\n",
    "                self.arm1_ang,\n",
    "                self.arm2_coords - self.target_coords,\n",
    "                self.arm2_ang,\n",
    "            )\n",
    "        elif symbol == pyglet.window.key.Q:\n",
    "            pyglet.clock.set_fps_limit(1000)\n",
    "        elif symbol == pyglet.window.key.A:\n",
    "            pyglet.clock.set_fps_limit(30)\n",
    "\n",
    "    def on_mouse_motion(self, x, y, dx, dy):\n",
    "        self.target_coords[:] = [x, y]\n",
    "\n",
    "    def on_mouse_enter(self, x, y):\n",
    "        self.mouse_in[0] = True\n",
    "\n",
    "    def on_mouse_leave(self, x, y):\n",
    "        self.mouse_in[0] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acQjzQRhQInD"
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T21:24:03.352154Z",
     "start_time": "2021-05-16T21:24:03.183285Z"
    },
    "id": "PiD_1LUgxwxs"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "\n",
    "class myEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"],\n",
    "                \"video.frames_per_second\": 50}\n",
    "\n",
    "    def __init__(self, threshold=10, target_coords=[250, 300], mode=\"hard\"):\n",
    "        self.state_dim = 7\n",
    "        self.dt = 0.1  # refresh rate\n",
    "\n",
    "        self.arm1_long = 100\n",
    "        self.arm2_long = 100\n",
    "        self.arm1_ang = 0\n",
    "        self.arm2_ang = 0\n",
    "        self.arm1_coords = np.array([0, 0])\n",
    "        self.arm2_coords = np.array([0, 0])\n",
    "\n",
    "        self.viewer = None\n",
    "        self.viewer_xy = (400, 400)\n",
    "        self.norm_dist = self.viewer_xy[0]/2.\n",
    "        self.got_target = False\n",
    "        self.steps_in_target = 0\n",
    "        self.threshold = threshold\n",
    "        self.mouse_in = np.array([False])\n",
    "        self.target_width = 15\n",
    "\n",
    "        self.action_space = Discrete(3 * 3)\n",
    "        self.observation_space = Box(\n",
    "            low=-1e10, high=1e10, shape=(self.state_dim,), dtype=np.float32\n",
    "        )\n",
    "        self.observation_space.n = self.state_dim\n",
    "        self.mode = mode\n",
    "\n",
    "        self.target_coords = np.array(target_coords)\n",
    "        self.target_coords_init = self.target_coords.copy()\n",
    "        self.center_coords = np.array(self.viewer_xy) / 2\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        if self.viewer is None:\n",
    "            self.viewer = Viewer(\n",
    "                *self.viewer_xy,\n",
    "                self.arm1_coords,\n",
    "                self.arm2_coords,\n",
    "                self.arm1_ang,\n",
    "                self.arm2_ang,\n",
    "                self.target_coords,\n",
    "                self.target_width,\n",
    "                self.mouse_in\n",
    "            )\n",
    "        self.viewer.render(\n",
    "            self.arm1_coords, self.arm2_coords, self.arm1_ang,\n",
    "            self.arm2_ang, self.target_coords\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        if self.mode == \"supereasy\":  # Do not reset anything\n",
    "            pass\n",
    "        elif self.mode == \"easy\":  # Reset arm to (0, 0)\n",
    "            self.arm1_ang = 0\n",
    "            self.arm2_ang = 0\n",
    "            self.arm1_coords = np.array([0, 0])\n",
    "            self.arm2_coords = np.array([0, 0])\n",
    "        elif self.mode == 'medium':  # Reset arm to random position\n",
    "            self.arm1_ang = np.random.uniform(0, 2*np.pi)\n",
    "            self.arm2_ang = np.random.uniform(0, 2*np.pi)\n",
    "            self.arm1_coords = np.random.randint(low=100, high=300, size=2)\n",
    "            self.arm2_coords = np.random.randint(low=100, high=300, size=2)\n",
    "        elif self.mode == \"hard\":  # Move target to random position\n",
    "            pxy = np.random.randint(low=100, high=300, size=2)\n",
    "            self.target_coords[:] = pxy\n",
    "        elif self.mode == \"follow\":  # Move target to random nearby position\n",
    "            self.target_coords += np.random.randint(low=-20, high=20, size=2)\n",
    "            self.target_coords = np.clip(self.target_coords, 100, 300)\n",
    "\n",
    "        s = self._get_state()\n",
    "        self.steps_in_target = 0\n",
    "        self.got_target = False\n",
    "\n",
    "        return s\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Returns internal state.\"\"\"\n",
    "        target_pos = (self.target_coords - self.center_coords)/self.norm_dist\n",
    "        arm1_dist = (self.target_coords - self.arm1_coords)/self.norm_dist\n",
    "        arm2_dist = (self.target_coords - self.arm2_coords)/self.norm_dist\n",
    "        has_target = 1.0 if self.steps_in_target > 0 else 0.0\n",
    "        return np.array([*target_pos, *arm1_dist, *arm2_dist, has_target])\n",
    "\n",
    "    def step(self, act):\n",
    "        action1 = act // 3 - 1\n",
    "        action2 = act % 3 - 1\n",
    "\n",
    "        self.arm1_ang += action1 * self.dt\n",
    "        self.arm1_ang %= np.pi * 2\n",
    "        self.arm2_ang += action2 * self.dt\n",
    "        self.arm2_ang %= np.pi * 2\n",
    "\n",
    "        self.arm1_coords = self.center_coords.copy()\n",
    "        self.arm1_coords[0] += self.arm1_long * np.cos(self.arm1_ang)\n",
    "        self.arm1_coords[1] += self.arm1_long * np.sin(self.arm1_ang)\n",
    "\n",
    "        self.arm2_coords = self.arm1_coords.copy()\n",
    "        self.arm2_coords[0] += self.arm2_long * \\\n",
    "            np.cos(self.arm1_ang + self.arm2_ang)\n",
    "        self.arm2_coords[1] += self.arm2_long * \\\n",
    "            np.sin(self.arm1_ang + self.arm2_ang)\n",
    "\n",
    "        s = self._get_state()\n",
    "\n",
    "        # Euclidean distance between target and arm2\n",
    "        dist = np.linalg.norm(self.arm2_coords - self.target_coords)\n",
    "        r = -dist/self.norm_dist\n",
    "\n",
    "        # Modify reward\n",
    "        if dist <= self.target_width:  # if touching the target\n",
    "            r += np.sqrt(2)\n",
    "            self.steps_in_target += 1\n",
    "            if self.steps_in_target > self.threshold:\n",
    "                r += 10\n",
    "                self.got_target = True\n",
    "        else:\n",
    "            self.steps_in_target = 0\n",
    "            self.got_target = False\n",
    "\n",
    "        return s, r, self.got_target, {}\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBGiW0ihQKfm"
   },
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T21:24:10.436856Z",
     "start_time": "2021-05-16T21:24:08.150506Z"
    },
    "id": "uSL88Tmnxwxv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from collections import deque\n",
    "import random\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_memory=100):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = []\n",
    "\n",
    "    def append(self, item):\n",
    "        self.memory.append(item)\n",
    "        if self.len() > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "def build_q_nnet(n_state, n_actions, alpha, lr_decay):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_shape=(n_state,),\n",
    "                    bias_initializer=\"RandomNormal\",\n",
    "                    activation=\"relu\"))\n",
    "    model.add(Dense(n_actions))\n",
    "\n",
    "    if lr_decay:\n",
    "        lr = ExponentialDecay(\n",
    "            alpha,\n",
    "            decay_steps=1000,\n",
    "            decay_rate=0.9,\n",
    "            staircase=True)\n",
    "    else:\n",
    "        lr = alpha\n",
    "\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=lr))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def replay(q_nnet, gamma, mem, n_actions, batch_size=100):\n",
    "    if batch_size <= 0:\n",
    "        return q_nnet\n",
    "\n",
    "    n_samples = min(batch_size, mem.len())\n",
    "    minibatch = random.sample(mem.memory, n_samples)\n",
    "    X_train = np.zeros((n_samples, minibatch[0][0].shape[0]))\n",
    "    y_train = np.zeros((n_samples, n_actions))\n",
    "\n",
    "    for i, (S, A, R, S_, done) in enumerate(minibatch):\n",
    "        if not done:\n",
    "            # next state is not terminal:\n",
    "            q_target = R + gamma*q_nnet.predict(S_.reshape(1, -1)).max()\n",
    "        else:\n",
    "            q_target = R  # next state is terminal\n",
    "\n",
    "        target_vector = q_nnet.predict(np.array(S.reshape(1, -1)))\n",
    "        target_vector[0][A] = q_target\n",
    "        q_nnet.fit(np.array(S.reshape(1, -1)),\n",
    "                   target_vector, verbose=0)  # update\n",
    "\n",
    "    return q_nnet\n",
    "\n",
    "\n",
    "def choose_action(state, nnet, epsilon):\n",
    "    state_actions = nnet.predict(state.reshape(1, -1))[0]\n",
    "    # act non-greedy or state-action have no value\n",
    "    if np.random.uniform() < epsilon or (state_actions == 0).all():\n",
    "        act = np.random.randint(len(state_actions))\n",
    "    else:   # act greedy\n",
    "        act = np.argmax(state_actions)\n",
    "    return act\n",
    "\n",
    "\n",
    "def rl(env, epsilon, alpha, gamma, n_episodes,\n",
    "       sleep_time=0.001, vis=True, max_steps=1000,\n",
    "       decay=0.99, min_epsilon=0.01, max_memory=5000, n_memory=500,\n",
    "       batch_size=100, q_nnet=None, lr_decay=False):\n",
    "    \"\"\"Perform reinforcement learning.\"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = env.state_dim\n",
    "\n",
    "    if q_nnet is None:\n",
    "        q_nnet = build_q_nnet(n_states, n_actions, alpha, lr_decay)\n",
    "\n",
    "    mem = Memory(max_memory)\n",
    "    steps_hist = []\n",
    "    rewards_hist = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        S = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            act = choose_action(S, q_nnet, epsilon)\n",
    "            # take action & get next state and reward\n",
    "            S_, R, done, _ = env.step(act)\n",
    "            total_reward += R\n",
    "            mem.append([S, act, R, S_, done])\n",
    "            S = S_  # move to next state\n",
    "\n",
    "            if (step + 1) % n_memory == 0:\n",
    "                q_nnet = replay(q_nnet, gamma, mem, n_actions, batch_size)\n",
    "\n",
    "            if vis and not done:\n",
    "                if (step + 1) % 100 == 0:\n",
    "                    print(f\"[Episodio {episode + 1}] Step {step + 1}\")\n",
    "                env.render()\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "            if done:\n",
    "                steps_hist.append(step + 1)\n",
    "                rewards_hist.append(total_reward)\n",
    "                print('Episodio %s: total steps = %s, total reward = %.2f' % (\n",
    "                    episode + 1, step + 1, total_reward))\n",
    "                break\n",
    "\n",
    "            elif step + 1 == max_steps:\n",
    "                print(\n",
    "                    f\"Episodio {episode + 1}: no alcanza tras {step + 1} pasos. Total reward = {total_reward:.2f}\")\n",
    "                \n",
    "        if epsilon > min_epsilon:\n",
    "            epsilon *= decay\n",
    "\n",
    "    return q_nnet, episode + 1, steps_hist, rewards_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Dx1zdS6QNct"
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoPuCoui_TBm",
    "outputId": "9f6b2e61-8906-4028-82d1-6ac3fff42cc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 1: no alcanza tras 400 pasos. Total reward = -358.75\n",
      "Episodio 2: no alcanza tras 400 pasos. Total reward = -147.48\n",
      "Episodio 3: no alcanza tras 400 pasos. Total reward = -93.25\n",
      "Episodio 4: no alcanza tras 400 pasos. Total reward = -95.16\n",
      "Episodio 5: no alcanza tras 400 pasos. Total reward = -42.86\n",
      "Episodio 6: no alcanza tras 400 pasos. Total reward = -160.89\n",
      "Episodio 7: no alcanza tras 400 pasos. Total reward = -136.29\n",
      "Episodio 8: no alcanza tras 400 pasos. Total reward = -225.05\n",
      "Episodio 9: no alcanza tras 400 pasos. Total reward = -339.29\n",
      "Episodio 10: no alcanza tras 400 pasos. Total reward = -46.14\n",
      "Episodio 11: total steps = 278, total reward = 57.36\n",
      "Episodio 12: no alcanza tras 400 pasos. Total reward = -166.13\n",
      "Episodio 13: no alcanza tras 400 pasos. Total reward = -190.78\n",
      "Episodio 14: no alcanza tras 400 pasos. Total reward = -80.13\n",
      "Episodio 15: total steps = 137, total reward = 45.41\n",
      "Episodio 16: no alcanza tras 400 pasos. Total reward = 38.05\n",
      "Episodio 17: no alcanza tras 400 pasos. Total reward = -5.84\n",
      "Episodio 18: total steps = 99, total reward = 13.72\n",
      "Episodio 19: no alcanza tras 400 pasos. Total reward = -11.90\n",
      "Episodio 20: no alcanza tras 400 pasos. Total reward = 63.84\n",
      "Episodio 21: no alcanza tras 400 pasos. Total reward = 5.01\n",
      "Episodio 22: no alcanza tras 400 pasos. Total reward = 6.47\n",
      "Episodio 23: no alcanza tras 400 pasos. Total reward = -157.78\n",
      "Episodio 24: no alcanza tras 400 pasos. Total reward = -51.15\n",
      "Episodio 25: no alcanza tras 400 pasos. Total reward = 10.55\n",
      "Episodio 26: total steps = 82, total reward = 7.60\n",
      "Episodio 27: no alcanza tras 400 pasos. Total reward = -232.67\n",
      "Episodio 28: no alcanza tras 400 pasos. Total reward = -125.85\n",
      "Episodio 29: no alcanza tras 400 pasos. Total reward = -2.12\n",
      "Episodio 30: no alcanza tras 400 pasos. Total reward = -64.51\n",
      "Episodio 31: no alcanza tras 400 pasos. Total reward = 24.01\n",
      "Episodio 32: no alcanza tras 400 pasos. Total reward = 44.48\n",
      "Episodio 33: no alcanza tras 400 pasos. Total reward = 10.06\n",
      "Episodio 34: no alcanza tras 400 pasos. Total reward = 7.42\n",
      "Episodio 35: total steps = 261, total reward = 48.26\n",
      "Episodio 36: total steps = 184, total reward = 51.10\n",
      "Episodio 37: no alcanza tras 400 pasos. Total reward = -34.18\n",
      "Episodio 38: no alcanza tras 400 pasos. Total reward = -44.62\n",
      "Episodio 39: no alcanza tras 400 pasos. Total reward = -144.53\n",
      "Episodio 40: no alcanza tras 400 pasos. Total reward = -94.94\n",
      "Episodio 41: no alcanza tras 400 pasos. Total reward = -84.26\n",
      "Episodio 42: no alcanza tras 400 pasos. Total reward = -58.26\n",
      "Episodio 43: no alcanza tras 400 pasos. Total reward = -276.21\n",
      "Episodio 44: total steps = 111, total reward = 32.72\n",
      "Episodio 45: total steps = 219, total reward = 4.74\n",
      "Episodio 46: no alcanza tras 400 pasos. Total reward = -154.42\n",
      "Episodio 47: total steps = 213, total reward = 36.40\n",
      "Episodio 48: no alcanza tras 400 pasos. Total reward = -179.60\n",
      "Episodio 49: no alcanza tras 400 pasos. Total reward = -146.55\n",
      "Episodio 50: no alcanza tras 400 pasos. Total reward = -54.62\n",
      "Episodio 51: no alcanza tras 400 pasos. Total reward = -46.09\n",
      "Episodio 52: total steps = 40, total reward = 16.10\n",
      "Episodio 53: total steps = 273, total reward = 73.19\n",
      "Episodio 54: total steps = 212, total reward = -13.09\n",
      "Episodio 55: no alcanza tras 400 pasos. Total reward = 91.66\n",
      "Episodio 56: no alcanza tras 400 pasos. Total reward = 143.84\n",
      "Episodio 57: no alcanza tras 400 pasos. Total reward = 28.10\n",
      "Episodio 58: no alcanza tras 400 pasos. Total reward = 44.69\n",
      "Episodio 59: no alcanza tras 400 pasos. Total reward = -60.37\n",
      "Episodio 60: no alcanza tras 400 pasos. Total reward = -88.60\n",
      "Episodio 61: no alcanza tras 400 pasos. Total reward = -68.29\n",
      "Episodio 62: no alcanza tras 400 pasos. Total reward = -148.11\n",
      "Episodio 63: total steps = 81, total reward = 24.97\n",
      "Episodio 64: total steps = 239, total reward = 63.18\n",
      "Episodio 65: total steps = 188, total reward = 94.27\n",
      "Episodio 66: no alcanza tras 400 pasos. Total reward = -77.68\n",
      "Episodio 67: total steps = 335, total reward = 8.38\n",
      "Episodio 68: total steps = 111, total reward = 10.22\n",
      "Episodio 69: total steps = 33, total reward = 35.70\n",
      "Episodio 70: total steps = 64, total reward = 35.12\n",
      "Episodio 71: total steps = 254, total reward = -21.29\n",
      "Episodio 72: no alcanza tras 400 pasos. Total reward = 72.14\n",
      "Episodio 73: total steps = 400, total reward = 9.72\n",
      "Episodio 74: total steps = 76, total reward = 33.72\n",
      "Episodio 75: no alcanza tras 400 pasos. Total reward = 51.83\n",
      "Episodio 76: no alcanza tras 400 pasos. Total reward = -192.67\n",
      "Episodio 77: no alcanza tras 400 pasos. Total reward = -118.03\n",
      "Episodio 78: no alcanza tras 400 pasos. Total reward = -41.95\n",
      "Episodio 79: no alcanza tras 400 pasos. Total reward = -17.78\n",
      "Episodio 80: no alcanza tras 400 pasos. Total reward = -20.99\n",
      "Episodio 81: no alcanza tras 400 pasos. Total reward = -42.77\n",
      "Episodio 82: no alcanza tras 400 pasos. Total reward = -35.75\n",
      "Episodio 83: total steps = 399, total reward = -75.35\n",
      "Episodio 84: no alcanza tras 400 pasos. Total reward = -158.65\n",
      "Episodio 85: total steps = 310, total reward = 101.61\n",
      "Episodio 86: no alcanza tras 400 pasos. Total reward = 2.29\n",
      "Episodio 87: total steps = 315, total reward = 119.86\n",
      "Episodio 88: total steps = 33, total reward = 17.84\n",
      "Episodio 89: total steps = 388, total reward = -3.78\n",
      "Episodio 90: no alcanza tras 400 pasos. Total reward = 6.82\n",
      "Episodio 91: total steps = 69, total reward = 15.77\n",
      "Episodio 92: no alcanza tras 400 pasos. Total reward = -187.05\n",
      "Episodio 93: total steps = 120, total reward = 48.13\n",
      "Episodio 94: total steps = 345, total reward = -13.60\n",
      "Episodio 95: total steps = 382, total reward = 108.73\n",
      "Episodio 96: total steps = 300, total reward = 139.89\n",
      "Episodio 97: no alcanza tras 400 pasos. Total reward = -57.26\n",
      "Episodio 98: no alcanza tras 400 pasos. Total reward = -156.77\n",
      "Episodio 99: total steps = 268, total reward = 31.50\n",
      "Episodio 100: no alcanza tras 400 pasos. Total reward = 33.71\n"
     ]
    }
   ],
   "source": [
    "env = myEnv(mode='hard')\n",
    "\n",
    "# Load pre-trained weights, or set to 'None' for building a new model\n",
    "model = load_model('q_nnet-200.h5')\n",
    "\n",
    "q_nnet, episodes, steps, rewards = rl(\n",
    "    env,\n",
    "    epsilon=0.7,\n",
    "    alpha=0.001,\n",
    "    lr_decay=False,\n",
    "    gamma=0.9,\n",
    "    n_episodes=100,\n",
    "    min_epsilon=0.1,\n",
    "    vis=False,\n",
    "    decay=0.99,\n",
    "    max_steps=400,\n",
    "    max_memory=5000,\n",
    "    n_memory=200,\n",
    "    batch_size=100,\n",
    "    q_nnet=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yWwwOV92Cx3U"
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "q_nnet.save('q_nnet-200.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipnELtSmOiV"
   },
   "source": [
    "## Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T21:39:45.302168Z",
     "start_time": "2021-05-16T21:28:04.306Z"
    },
    "id": "IlJDB6dKccUY"
   },
   "outputs": [],
   "source": [
    "def test_policy(env, q_nnet, vis=True,\n",
    "                sleep_time=0.001,\n",
    "                max_steps=1000, verbose=True):\n",
    "    S = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        act = np.argmax(q_nnet.predict(S.reshape(1, -1))[0])\n",
    "        S_, R, done, _ = env.step(act)\n",
    "        total_reward += R\n",
    "        S = S_  # move to next state\n",
    "\n",
    "        if vis and not done:\n",
    "            if step + 1 % 100 == 0 and verbose:\n",
    "                print(f\"[test_policy] Step {step + 1}\")\n",
    "            env.render()\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"[test_policy] Alcanza objetivo tras {step + 1} pasos\")\n",
    "            break\n",
    "        elif step + 1 == max_steps and verbose:\n",
    "            print(f\"[test_policy] No alcanza objetivo tras {step + 1} pasos\")\n",
    "\n",
    "    return total_reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mivRkia1mjHt",
    "outputId": "56b51598-5cc5-4ed5-dc36-80f5a27c99c8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test_policy] Alcanza objetivo tras 36 pasos\n",
      "Refuerzo total política: 6.52\n"
     ]
    }
   ],
   "source": [
    "q_nnet = load_model('q_nnet-200.h5')\n",
    "env2 = myEnv(mode='hard')\n",
    "total_R, _ = test_policy(env2, q_nnet, vis=False)\n",
    "print(f\"Refuerzo total política: {total_R:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Practica_2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "bibliography.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
